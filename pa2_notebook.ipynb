{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from loader import load_instances, load_key\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.wsd import lesk\n",
    "import random\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load instances and key data in another cell\n",
    "data_f = 'multilingual-all-words.en.xml'  \n",
    "key_f = 'wordnet.en.key'\n",
    "\n",
    "# Call your loading functions\n",
    "dev_instances, test_instances = load_instances(data_f)\n",
    "dev_key, test_key = load_key(key_f)\n",
    "\n",
    "dev_instances = {k:v for (k,v) in dev_instances.items() if k in dev_key}\n",
    "test_instances = {k:v for (k,v) in test_instances.items() if k in test_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['it show the outline for a possible deal , in which industrial_nation would collectively cut their emission by @card@ by @card@ to @card@ percent compare with @card@ level , while major developing_country would reduce theirs during the same period by @card@ to @card@ percent .'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df[dev_df['Instance ID'] == 'd001.s009.t008']['Combined Context'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_and_lemmatize(word):\n",
    "    # Remove punctuation\n",
    "    word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Lemmatize\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "def create_dataframe(instances, key_dict):\n",
    "    data = []\n",
    "    for instance_id, instance in instances.items():\n",
    "        # Decode lemma and context if in byte format\n",
    "        lemma = instance.lemma.decode('utf-8') if isinstance(instance.lemma, bytes) else instance.lemma\n",
    "        context = [word.decode('utf-8') if isinstance(word, bytes) else word for word in instance.context]\n",
    "\n",
    "        # Retrieve the sense key(s) from key_dict, or None if not found\n",
    "        sense_key = key_dict.get(instance_id, [None])\n",
    "        \n",
    "        # Append the processed data\n",
    "        data.append({\n",
    "            'Instance ID': instance_id,             \n",
    "            'Lemma': lemma,                         \n",
    "            'Original Context': context,\n",
    "            'Combined Context': ' '.join(context),          \n",
    "            'Index': instance.index,               \n",
    "            'Sense Key': sense_key                  \n",
    "        })\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = create_dataframe(dev_instances, dev_key)\n",
    "test_df = create_dataframe(test_instances, test_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_context(context):\n",
    "    processed_context = []\n",
    "    for word in context:\n",
    "        # Convert to lowercase\n",
    "        word = word.lower()\n",
    "\n",
    "        # Handle \"@card@\" tokens and numeric values by replacing with \"NUM\"\n",
    "        if word == \"@card@\" or re.fullmatch(r'\\d+', word):\n",
    "            processed_context.append(\"NUM\")\n",
    "            continue\n",
    "        \n",
    "        # Preserve periods within abbreviations and replace with underscores (e.g., \"u.n.\" -> \"u_n\")\n",
    "        word = re.sub(r'\\b(\\w\\.)+', lambda match: match.group(0).replace('.', '_'), word)\n",
    "        \n",
    "        # Split hyphenated compound words (e.g., \"u_n-sponsored\" -> [\"u_n\", \"sponsored\"])\n",
    "        parts = re.split(r'-(?=\\w)', word)\n",
    "        \n",
    "        # Process each part separately\n",
    "        for part in parts:\n",
    "            # Remove isolated punctuation from each part\n",
    "            part = part.strip(string.punctuation)\n",
    "            \n",
    "            # Lemmatize, remove stop words, and add to processed context if not empty\n",
    "            \n",
    "            # if part and part not in stop_words:\n",
    "            #     processed_context.append(lemmatizer.lemmatize(part))\n",
    "                \n",
    "            if part:\n",
    "                processed_context.append(lemmatizer.lemmatize(part))\n",
    "    \n",
    "    return processed_context\n",
    "\n",
    "# Example application\n",
    "dev_df['Modified Context'] = dev_df['Original Context'].apply(preprocess_context)\n",
    "test_df['Modified Context'] = test_df['Original Context'].apply(preprocess_context)\n",
    "\n",
    "dev_df['Combined Modified Context'] = dev_df['Modified Context'].apply(lambda x: ' '.join(x))\n",
    "test_df['Combined Modified Context'] = test_df['Modified Context'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predicted_synsets, actual_synsets):\n",
    "\n",
    "    correct_predictions = 0\n",
    "    correct_indices = []\n",
    "    incorrect_indices = []\n",
    "    \n",
    "    for i, (predicted, actual) in enumerate(zip(predicted_synsets, actual_synsets)):\n",
    "        if predicted == actual:\n",
    "            correct_predictions += 1\n",
    "            correct_indices.append(i)\n",
    "        else:\n",
    "            incorrect_indices.append(i)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / len(actual_synsets)\n",
    "    return accuracy, correct_indices, incorrect_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lesk Accuracy: 35.57%\n",
      "Baseline Accuracy: 65.46%\n"
     ]
    }
   ],
   "source": [
    "df = dev_df\n",
    "\n",
    "actual_synsets = [wn.lemma_from_key(row['Sense Key'][0]).synset() for _, row in df.iterrows()]\n",
    "\n",
    "# Generate predicted labels using Lesk\n",
    "lesk_predictions = [lesk(row['Modified Context'], row['Lemma']) for _, row in df.iterrows()]\n",
    "\n",
    "baseline_predictions = [wn.synsets(row['Lemma'])[0] if wn.synsets(row['Lemma']) else None for _, row in df.iterrows()]\n",
    "\n",
    "# Calculate accuracy and get indices for correct and incorrect predictions\n",
    "lesk_accuracy, lesk_correct_indices, lesk_incorrect_indices = calculate_accuracy(lesk_predictions, actual_synsets)\n",
    "\n",
    "baseline_accuracy, baseline_correct_indices, baseline_incorrect_indices = calculate_accuracy(baseline_predictions, actual_synsets)\n",
    "\n",
    "# Print rounded accuracy values\n",
    "print(f\"Lesk Accuracy: {lesk_accuracy*100:.2f}%\")\n",
    "print(f\"Baseline Accuracy: {baseline_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------- Correct Prediction Debug -----------------------------------------\n",
      "\n",
      "Instance ID: d001.s020.t005\n",
      "Ambiguous word: climate_change\n",
      "\n",
      "Context (lemmatized): ['this', 'be', 'clearly', 'a', 'game', 'where', 'a', 'new', 'economic', 'hegemony', 'be', 'be', 'develop', 'say', 'ulate', 'who', 'also', 'serve', 'a', 'the', 'regional', 'mexico', 'and', 'central_america', 'climate_change', 'adviser', 'for', 'conservation_international']\n",
      "\n",
      "Predicted Synset: Synset('climate_change.n.01')\n",
      "Definition of Predicted Synset: a change in the world's climate\n",
      "Actual Synset: Synset('climate_change.n.01')\n",
      "Definition of Actual Synset: a change in the world's climate\n",
      "\n",
      "Number of possible synsets: 1\n",
      "\n",
      "All possible synsets and their overlap scores:\n",
      "\n",
      "Synset: Synset('climate_change.n.01')\n",
      "Definition: a change in the world's climate\n",
      "Overlap Words: {'a', 'the'}\n",
      "Overlap Score: 2\n",
      "\n",
      "\n",
      "----------------------------------------- Incorrect Prediction Debug -----------------------------------------\n",
      "\n",
      "Instance ID: d001.s019.t003\n",
      "Ambiguous word: cost\n",
      "\n",
      "Context (lemmatized): ['ricardo', 'ulate', 'a', 'costa', 'rican', 'delegate', 'say', 'it', 'be', 'not', 'surprising', 'that', 'the', 'major_power', 'be', 'fight', 'over', 'who', 'should', 'bear', 'the', 'cost', 'for', 'curb', 'greenhouse_gas', 'even', 'a', 'vulnerable', 'country', 'have', 'become', 'more', 'aggressive', 'in', 'seek', 'to', 'hold', 'the', 'big', 'emitter', 'accountable', 'for', 'their', 'action']\n",
      "\n",
      "Predicted Synset: Synset('price.n.03')\n",
      "Definition of Predicted Synset: value measured by what must be given or done or undergone to obtain something\n",
      "Actual Synset: Synset('cost.n.01')\n",
      "Definition of Actual Synset: the total spent for goods or services including money and time and labor\n",
      "\n",
      "Number of possible synsets: 5\n",
      "\n",
      "All possible synsets and their overlap scores:\n",
      "\n",
      "Synset: Synset('cost.n.01')\n",
      "Definition: the total spent for goods or services including money and time and labor\n",
      "Overlap Words: {'for', 'the'}\n",
      "Overlap Score: 2\n",
      "\n",
      "Synset: Synset('monetary_value.n.01')\n",
      "Definition: the property of having material worth (often indicated by the amount of money something would bring if sold)\n",
      "Overlap Words: {'the'}\n",
      "Overlap Score: 1\n",
      "\n",
      "Synset: Synset('price.n.03')\n",
      "Definition: value measured by what must be given or done or undergone to obtain something\n",
      "Overlap Words: {'to', 'be'}\n",
      "Overlap Score: 2\n",
      "\n",
      "Synset: Synset('cost.v.01')\n",
      "Definition: be priced at\n",
      "Overlap Words: {'be'}\n",
      "Overlap Score: 1\n",
      "\n",
      "Synset: Synset('cost.v.02')\n",
      "Definition: require to lose, suffer, or sacrifice\n",
      "Overlap Words: {'to'}\n",
      "Overlap Score: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lesk_with_full_debug(context, word, actual_synset):\n",
    "    print(f\"Ambiguous word: {word}\")\n",
    "    print(f\"\\nContext (lemmatized): {context}\")\n",
    "    \n",
    "    # Run NLTK's Lesk algorithm and get the predicted synset\n",
    "    predicted_synset = lesk(context, word)\n",
    "    print(f\"\\nPredicted Synset: {predicted_synset}\")\n",
    "    print(f\"Definition of Predicted Synset: {predicted_synset.definition() if predicted_synset else 'None'}\")\n",
    "    \n",
    "    # Print the actual synset for comparison\n",
    "    print(f\"Actual Synset: {actual_synset}\")\n",
    "    print(f\"Definition of Actual Synset: {actual_synset.definition() if actual_synset else 'None'}\")\n",
    "    \n",
    "    print(\"\\nNumber of possible synsets:\", len(wn.synsets(word)))\n",
    "    \n",
    "    print(\"\\nAll possible synsets and their overlap scores:\\n\")\n",
    "\n",
    "    # Calculate and display overlap for each synset of the ambiguous word\n",
    "    lemmatized_context = set(context)\n",
    "    for synset in wn.synsets(word):\n",
    "        # Gloss, examples, and hypernyms words\n",
    "        gloss_words = set(synset.definition().split())\n",
    "        \n",
    "        overlap_words = lemmatized_context.intersection(gloss_words)\n",
    "        overlap_score = len(overlap_words)\n",
    "        \n",
    "        # Display details for this synset\n",
    "        print(f\"Synset: {synset}\")\n",
    "        print(f\"Definition: {synset.definition()}\")\n",
    "        print(f\"Overlap Words: {overlap_words}\")\n",
    "        print(f\"Overlap Score: {overlap_score}\\n\")\n",
    "\n",
    "    return predicted_synset\n",
    "\n",
    "correct_samples = random.sample(lesk_correct_indices, 1) \n",
    "incorrect_samples = random.sample(lesk_incorrect_indices, 1)  \n",
    "\n",
    "# Display Lesk's computation for correct predictions\n",
    "print(\"\\n----------------------------------------- Correct Prediction Debug -----------------------------------------\")\n",
    "for idx in correct_samples:\n",
    "    row = df.iloc[idx]\n",
    "    actual_synset = actual_synsets[idx]\n",
    "    print(f\"\\nInstance ID: {row['Instance ID']}\")\n",
    "    lesk_with_full_debug(row['Modified Context'], row['Lemma'], actual_synset)\n",
    "\n",
    "# Display Lesk's computation for incorrect predictions\n",
    "print(\"\\n----------------------------------------- Incorrect Prediction Debug -----------------------------------------\")\n",
    "for idx in incorrect_samples:\n",
    "    row = df.iloc[idx]\n",
    "    actual_synset = actual_synsets[idx]\n",
    "    print(f\"\\nInstance ID: {row['Instance ID']}\")\n",
    "    lesk_with_full_debug(row['Modified Context'], row['Lemma'], actual_synset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aidanlicoppe/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in a sentence in the form of a string and returns it lemmatized with stop words and punctuation removed\n",
    "def process_sentence(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # Lemmatize each token and remove stop words and punctuation\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    \n",
    "    # Join the lemmatized tokens into a single sentence\n",
    "    lemmatized_sentence = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence):\n",
    "    # Tokenize the sentence and get the embeddings for all tokens\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # Take the mean of the token embeddings to represent the sentence\n",
    "    sentence_embedding = last_hidden_states.mean(dim=1).squeeze()\n",
    "    return sentence_embedding.detach().numpy()\n",
    "\n",
    "def get_synset_embeddings(target_word, comparison_type='definition'):\n",
    "    # Create a dictionary to store sentence embeddings for each synset example\n",
    "    synset_embeddings = {}\n",
    "    for synset in wn.synsets(target_word):\n",
    "        # Use the first example sentence of the synset if available\n",
    "        if comparison_type == 'examples':\n",
    "            if synset.examples():\n",
    "                example_sentence = synset.examples()[0]\n",
    "                # print(f\"Example sentence for synset '{synset}': {example_sentence}\")\n",
    "                # Get the sentence-level embedding\n",
    "                synset_embedding = get_sentence_embedding(example_sentence)\n",
    "                synset_embeddings[synset] = synset_embedding\n",
    "        if comparison_type == 'definition':\n",
    "            if synset.definition():\n",
    "                definition_sentence = synset.definition()\n",
    "                # definition_sentence = process_sentence(definition_sentence)\n",
    "                # Get the sentence-level embedding\n",
    "                synset_embedding = get_sentence_embedding(definition_sentence)\n",
    "                synset_embeddings[synset] = synset_embedding\n",
    "    return synset_embeddings\n",
    "\n",
    "\n",
    "def predict_synset_BERT(sentences, target_word, metric='cosine', comparison_type='definition'):\n",
    "    \"\"\"\n",
    "    Predict the most likely synset for each sentence based on the target word's context.\n",
    "\n",
    "    Parameters:\n",
    "    - sentences: list of sentences containing the target word.\n",
    "    - target_word: the word to disambiguate.\n",
    "    - metric: the type of distance metric to use ('cosine' for cosine similarity, 'euclidean' for Euclidean distance).\n",
    "\n",
    "    Returns:\n",
    "    - predicted_synsets: a list of the predicted synsets for each sentence.\n",
    "    \"\"\"\n",
    "    # Get synset sentence embeddings for the target word\n",
    "    synset_embeddings = get_synset_embeddings(target_word, comparison_type=comparison_type)\n",
    "    \n",
    "    predicted_synsets = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # sentence = process_sentence(sentence)\n",
    "        # Get the sentence-level embedding for the input sentence\n",
    "        sentence_embedding = get_sentence_embedding(sentence)\n",
    "        \n",
    "        # Initialize variables to track the best matching synset\n",
    "        best_synset = None\n",
    "        if metric == 'cosine':\n",
    "            best_score = -1\n",
    "        elif metric == 'euclidean':\n",
    "            best_score = float('inf')\n",
    "        \n",
    "        # Compare with each synset embedding based on the chosen metric\n",
    "        for synset, synset_embedding in synset_embeddings.items():\n",
    "            if synset_embedding is not None:\n",
    "                # Calculate similarity or distance based on the metric\n",
    "                if metric == 'cosine':\n",
    "                    score = 1 - cosine(sentence_embedding, synset_embedding)  # Higher is better for cosine similarity\n",
    "                elif metric == 'euclidean':\n",
    "                    score = euclidean(sentence_embedding, synset_embedding)   # Lower is better for Euclidean distance\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported metric. Choose either 'cosine' or 'euclidean'.\")\n",
    "                \n",
    "                # Determine best score and synset based on the metric\n",
    "                if ((metric == 'cosine' and score > best_score) or (metric == 'euclidean' and score < best_score)):\n",
    "                    best_score = score\n",
    "                    best_synset = synset\n",
    "                    \n",
    "        if best_synset is None:\n",
    "            best_synset = wn.synsets(target_word)[0]\n",
    "        \n",
    "        # Append the best matching synset for the sentence\n",
    "        predicted_synsets.append(best_synset)\n",
    "    \n",
    "    return predicted_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df\n",
    "\n",
    "# Extract sentences and lemmas\n",
    "sentences = df['Combined Context'].tolist()\n",
    "lemmas = df['Lemma'].tolist()\n",
    "\n",
    "# Initialize an array to store the predicted synsets\n",
    "predicted_synsets_cosine = []\n",
    "predicted_synsets_euclidean = []\n",
    "\n",
    "# Run the prediction for each sentence and lemma pair\n",
    "for sentence, lemma in zip(sentences, lemmas):\n",
    "    # Run predict_synset_BERT on each sentence-lemma pair individually\n",
    "    synset_prediction_cosine = predict_synset_BERT([sentence], lemma, metric='cosine')\n",
    "    # Extract the predicted synset from the list and append\n",
    "    predicted_synsets_cosine.append(synset_prediction_cosine[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for cosine similarity: 46.90%\n"
     ]
    }
   ],
   "source": [
    "actual_synsets = [wn.lemma_from_key(row['Sense Key'][0]).synset() for _, row in df.iterrows()]\n",
    "\n",
    "acc_cos, correct_cos, incorrect_cos = calculate_accuracy(predicted_synsets_cosine, actual_synsets)\n",
    "\n",
    "print(f\"Accuracy for cosine similarity: {acc_cos*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------- Correct Prediction Debug -----------------------------------------\n",
      "\n",
      "Instance ID: d008.s001.t001\n",
      "Ambiguous word: microbe\n",
      "\n",
      "Context (lemmatized): Arsenic-tolerant microbe raise possibility of new type of life_on_earth , or elsewhere\n",
      "\n",
      "Predicted Synset: Synset('microbe.n.01')\n",
      "Definition of Predicted Synset: a minute life form (especially a disease-causing bacterium); the term is not in technical use\n",
      "Actual Synset: Synset('microbe.n.01')\n",
      "Definition of Actual Synset: a minute life form (especially a disease-causing bacterium); the term is not in technical use\n",
      "\n",
      "Number of possible synsets: 1\n",
      "\n",
      "All possible synsets and their cosine similarity:\n",
      "\n",
      "Definition sentence for synset 'Synset('microbe.n.01')': a minute life form (especially a disease-causing bacterium); the term is not in technical use\n",
      "Synset: Synset('microbe.n.01'), Cosine Similarity: 0.6752355098724365\n",
      "\n",
      "\n",
      "----------------------------------------- Incorrect Prediction Debug -----------------------------------------\n",
      "\n",
      "Instance ID: d007.s013.t006\n",
      "Ambiguous word: advantage\n",
      "\n",
      "Context (lemmatized): another of these advantage to which it be allude to with the fare-play be to avoid that the team of the lawbreaker player take advantage unjustly of the opposite team , which do not happen either .\n",
      "\n",
      "Predicted Synset: Synset('advantage.n.01')\n",
      "Definition of Predicted Synset: the quality of having a superior or more favorable position\n",
      "Actual Synset: Synset('advantage.n.03')\n",
      "Definition of Actual Synset: benefit resulting from some event or action\n",
      "\n",
      "Number of possible synsets: 4\n",
      "\n",
      "All possible synsets and their cosine similarity:\n",
      "\n",
      "Definition sentence for synset 'Synset('advantage.n.01')': the quality of having a superior or more favorable position\n",
      "Synset: Synset('advantage.n.01'), Cosine Similarity: 0.6402822136878967\n",
      "\n",
      "Definition sentence for synset 'Synset('advantage.n.02')': (tennis) first point scored after deuce\n",
      "Synset: Synset('advantage.n.02'), Cosine Similarity: 0.45941001176834106\n",
      "\n",
      "Definition sentence for synset 'Synset('advantage.n.03')': benefit resulting from some event or action\n",
      "Synset: Synset('advantage.n.03'), Cosine Similarity: 0.5636229515075684\n",
      "\n",
      "Definition sentence for synset 'Synset('advantage.v.01')': give an advantage to\n",
      "Synset: Synset('advantage.v.01'), Cosine Similarity: 0.5517401099205017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def BERT_with_full_debug(context, word, actual_synset):\n",
    "    print(f\"Ambiguous word: {word}\")\n",
    "    print(f\"\\nContext (lemmatized): {context}\")\n",
    "    \n",
    "    # Run NLTK's Lesk algorithm and get the predicted synset\n",
    "    predicted_synset = predict_synset_BERT([context], word, metric='cosine')\n",
    "    predicted_synset = predicted_synset[0]\n",
    "    print(f\"\\nPredicted Synset: {predicted_synset}\")\n",
    "    print(f\"Definition of Predicted Synset: {predicted_synset.definition() if predicted_synset else 'None'}\")\n",
    "    \n",
    "    # Print the actual synset for comparison\n",
    "    print(f\"Actual Synset: {actual_synset}\")\n",
    "    print(f\"Definition of Actual Synset: {actual_synset.definition() if actual_synset else 'None'}\")\n",
    "    \n",
    "    print(\"\\nNumber of possible synsets:\", len(wn.synsets(word)))\n",
    "    \n",
    "    print(\"\\nAll possible synsets and their cosine similarity:\\n\")\n",
    "\n",
    "    for synset in wn.synsets(word):\n",
    "        # Print definition sentence\n",
    "        definition_sentence = synset.definition()\n",
    "        print(f\"Definition sentence for synset '{synset}': {definition_sentence}\")\n",
    "        \n",
    "        # Get the sentence-level embedding\n",
    "        synset_embedding = get_sentence_embedding(definition_sentence)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = 1 - cosine(get_sentence_embedding(context), synset_embedding)\n",
    "        \n",
    "        print(f\"Synset: {synset}, Cosine Similarity: {similarity}\\n\")\n",
    "\n",
    "    return predicted_synset\n",
    "\n",
    "correct_samples = random.sample(correct_cos, 1) \n",
    "incorrect_samples = random.sample(incorrect_cos, 1)  \n",
    "\n",
    "print(\"\\n----------------------------------------- Correct Prediction Debug -----------------------------------------\")\n",
    "for idx in correct_samples:\n",
    "    row = df.iloc[idx]\n",
    "    actual_synset = actual_synsets[idx]\n",
    "    print(f\"\\nInstance ID: {row['Instance ID']}\")\n",
    "    BERT_with_full_debug(row['Combined Context'], row['Lemma'], actual_synset)\n",
    "    \n",
    "print(\"\\n----------------------------------------- Incorrect Prediction Debug -----------------------------------------\")\n",
    "for idx in incorrect_samples:\n",
    "    row = df.iloc[idx]\n",
    "    actual_synset = actual_synsets[idx]\n",
    "    print(f\"\\nInstance ID: {row['Instance ID']}\")\n",
    "    BERT_with_full_debug(row['Combined Context'], row['Lemma'], actual_synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
